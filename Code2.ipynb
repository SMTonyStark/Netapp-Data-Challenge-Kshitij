{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import math\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",dtype=object,na_values=str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV'\n",
      "  'She left her husband. He killed their children. Just another day in America.']\n",
      " [\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\"\n",
      "  'Of course it has a song.']\n",
      " ['Hugh Grant Marries For The First Time At Age 57'\n",
      "  'The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.']\n",
      " [\"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\"\n",
      "  'The actor gives Dems an ass-kicking for not fighting hard enough against Donald Trump.']]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(data[:,2:4])\n",
    "y = np.array(data[:,1])\n",
    "print(x[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = []\n",
    "for i in range(0,x.shape[0]):\n",
    "    for sent in x[i]:\n",
    "        if np.nan_to_num(sent) == 0:     # If short description is missing\n",
    "            corpus_raw.append(\"NA\")      # Necessary, otherwise while feeding training data into NN can cause mismatch\n",
    "        elif np.nan_to_num(sent) != 0:\n",
    "            corpus_raw.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV', 'She left her husband. He killed their children. Just another day in America.', \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\", 'Of course it has a song.', 'Hugh Grant Marries For The First Time At Age 57']\n",
      "(401664,)\n",
      "<class 'str'>\n",
      "NA\n"
     ]
    }
   ],
   "source": [
    "print(corpus_raw[0:5])\n",
    "print(np.shape(corpus_raw))\n",
    "print(type(corpus_raw[24199]))\n",
    "#words = corpus_raw[0].split()\n",
    "#print(words)\n",
    "print(np.nan_to_num(corpus_raw[24199]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words\n",
    "words2 = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "i = 0\n",
    "for sent in corpus_raw:\n",
    "    #words.append(sent.split())\n",
    "    #i = i + 1\n",
    "    #print(i)\n",
    "    for word in tokenizer.tokenize(sent):\n",
    "        words2.append(word.lower())\n",
    "    \n",
    "# for i in range(0,len(corpus_raw)):\n",
    "#     for word in corpus_raw[i].split():\n",
    "#         if word != '.':       # because we don't want to treat . as a word\n",
    "#             words.append(word)\n",
    "#print(type(words))\n",
    "#words = np.array(words)\n",
    "#print(type(words))\n",
    "#print(len(words))\n",
    "#print(words[0])\n",
    "\n",
    "Vocab = set(words2)    # so that all duplicate words are removed\n",
    "vocab = list(Vocab)\n",
    "# vocab = []\n",
    "# for j in words:\n",
    "#     if j not in vocab:\n",
    "#         vocab.append(j)\n",
    "\n",
    "\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(vocab)  # gives the total number of unique words\n",
    "\n",
    "for i,word in enumerate(vocab):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists of words\n",
    "words = []\n",
    "sentences = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "i = 0\n",
    "for sent in corpus_raw:\n",
    "    #words.append(sent.split())\n",
    "    #i = i + 1\n",
    "    #print(i)\n",
    "    for word in tokenizer.tokenize(sent):\n",
    "        words.append(word.lower())\n",
    "    sentences.append(words)\n",
    "    words = []\n",
    "# for i in range(0,len(corpus_raw)):\n",
    "#     for word in corpus_raw[i].split():\n",
    "#         if word != '.':       # because we don't want to treat . as a word\n",
    "#             words.append(word)\n",
    "#print(type(words))\n",
    "#words = np.array(words)\n",
    "#print(type(words))\n",
    "#print(len(words))\n",
    "#print(words[0])\n",
    "\n",
    "#Vocab = set(words)    # so that all duplicate words are removed\n",
    "#vocab = list(Vocab)\n",
    "# vocab = []\n",
    "# for j in words:\n",
    "#     if j not in vocab:\n",
    "#         vocab.append(j)\n",
    "\n",
    "\n",
    "# word2int = {}\n",
    "# int2word = {}\n",
    "# vocab_size = len(vocab)  # gives the total number of unique words\n",
    "\n",
    "# for i,word in enumerate(vocab):\n",
    "#     word2int[word] = i\n",
    "#     int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "86718\n",
      "[]\n",
      "<class 'list'>\n",
      "86718\n",
      "86718\n",
      "401664\n",
      "[['there', 'were', '2', 'mass', 'shootings', 'in', 'texas', 'last', 'week', 'but', 'only', '1', 'on', 'tv'], ['she', 'left', 'her', 'husband', 'he', 'killed', 'their', 'children', 'just', 'another', 'day', 'in', 'america'], ['will', 'smith', 'joins', 'diplo', 'and', 'nicky', 'jam', 'for', 'the', '2018', 'world', 'cup', 's', 'official', 'song'], ['of', 'course', 'it', 'has', 'a', 'song'], ['hugh', 'grant', 'marries', 'for', 'the', 'first', 'time', 'at', 'age', '57']]\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(vocab_size)\n",
    "print(words[0:100])\n",
    "print(type(vocab))\n",
    "#Vocab = list(vocab)\n",
    "#print(vocab[0:100])\n",
    "# for i in range(0,5):\n",
    "#     print(i)\n",
    "# for sent in corpus_raw:\n",
    "#     print(type(sent))\n",
    "print(len(word2int))\n",
    "print(len(int2word))\n",
    "#print(int2word)\n",
    "print(len(sentences))\n",
    "print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49854872, 60996040)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec = Word2Vec(sentences, size=100, window=5, min_count=0,workers=10,sg=0)\n",
    "model_word2vec.train(sentences,total_examples=len(sentences),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=86718, size=100, alpha=0.025)\n",
      "[ 0.15428673  0.3032078  -1.1721619   1.874103   -1.5606085  -0.24525835\n",
      "  2.2021377  -2.475612   -1.7615445   2.326912   -0.34606314 -1.062049\n",
      " -2.2459078  -0.15821838 -0.0905036   0.8615143  -2.3459883  -1.5937464\n",
      " -0.3024151   0.47964537 -0.29445362  0.11982829  0.3789861   1.4195501\n",
      " -0.38800597 -0.37164438  1.7658195  -0.5426802   0.27196807 -0.8482216\n",
      "  0.53250164 -0.33730203 -0.6293161  -0.3897586   2.6819797  -0.40592122\n",
      " -0.14170778 -0.66269076  0.7727368  -2.8266966  -0.35886714 -0.7655429\n",
      " -2.1962283  -2.2179437  -2.791443   -1.0361419   1.4445623   0.84580624\n",
      " -0.05743724 -0.4303834  -0.47139132  0.29042226 -1.8581903  -1.1161158\n",
      " -0.01754724  1.4625732   2.1342294  -0.40597525 -0.37957555  0.5402142\n",
      " -0.8409273   0.12632468  0.8405126   1.2006891   2.2413278  -0.64666384\n",
      " -0.3393913   0.21993287 -0.9356112   0.19827822 -0.7891616  -1.3792859\n",
      "  0.51050097 -0.04699465  0.51752055 -0.20769632  0.81789476 -0.4723141\n",
      " -2.1866217  -0.57456815  1.8308525   0.36720723 -2.227297    2.3169405\n",
      "  1.1242387  -1.961003   -0.9348298   0.7909439  -0.55487615 -1.7964721\n",
      "  0.3012279   0.39903513  0.7006815   2.5910192  -1.6795545   1.0464658\n",
      "  0.18418832  0.47919834 -0.67648983 -2.7292597 ]\n",
      "[('shooting', 0.7852242588996887), ('killings', 0.7188578844070435), ('shooters', 0.6943228244781494), ('incarceration', 0.6891462206840515), ('massacre', 0.6837863326072693), ('newtown', 0.6685141324996948), ('mass', 0.6398292779922485), ('edgartown', 0.6281864643096924), ('aurora', 0.6132979989051819), ('rampage', 0.6017170548439026)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model_word2vec)\n",
    "print(model_word2vec['killing'])\n",
    "print(model_word2vec.wv.most_similar(\"shootings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.save(\"Saved_model_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=86718, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"Saved_model_word2vec\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "embeddings = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(86718, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01042868  0.02176324 -0.02250957 -0.06258629  0.00782776  0.02794897\n",
      "  0.00023065 -0.0720012   0.01331906  0.0220496  -0.0013377  -0.02216832\n",
      " -0.02165697  0.06010749 -0.0029701   0.03460391 -0.07234558  0.05033163\n",
      " -0.00130906  0.07243614  0.05886856 -0.00194149  0.01002414  0.02834561\n",
      "  0.02269987 -0.01992226 -0.05661791 -0.04028002  0.08232529  0.01287358\n",
      " -0.00188548 -0.03678086 -0.15531382  0.00495727  0.00194723  0.01899534\n",
      " -0.03533955 -0.00352425 -0.06034484  0.0072311   0.00225596  0.07923514\n",
      " -0.02116636 -0.02062715 -0.00707681 -0.02920688  0.08443666 -0.04279945\n",
      "  0.01290212  0.01215223 -0.04320362 -0.01869603 -0.03856771  0.02556449\n",
      " -0.11824536  0.09582601 -0.09653168  0.01564028 -0.0628624   0.03083246\n",
      "  0.03544985 -0.05076908  0.00566671  0.02384304  0.0216185  -0.03771033\n",
      "  0.04122637  0.00288024 -0.02996035 -0.0177307   0.06270609 -0.03524997\n",
      " -0.00827872 -0.0556549  -0.01649423 -0.04432337  0.03637278  0.02263447\n",
      "  0.03852658 -0.00167972  0.05346249 -0.01165624  0.06038767  0.05675099\n",
      "  0.01739617 -0.03106136 -0.0229073   0.05699635 -0.00399931  0.0510834\n",
      "  0.06919824  0.03576701  0.03571143 -0.08222751  0.04651167  0.07020038\n",
      "  0.0515786   0.05587643  0.02314296 -0.02827143]\n",
      "[-1.8612489  -0.2885676  -1.4941822   3.2265735  -0.08313849 -3.126247\n",
      "  0.22613381 -0.74041677 -0.41081485  2.5019407  -1.2451053   0.22269614\n",
      "  0.16604857  1.6952585   2.647251    1.6176267   2.55638    -0.7695693\n",
      " -2.9931254   1.4478403  -3.0225234  -2.7241569  -0.3744455   0.5215901\n",
      " -3.2990131   2.18888    -1.9480959   3.027542   -0.06710951  1.9445344\n",
      "  0.49774876  1.0684828  -1.189954   -2.544733    0.57041895 -3.3660605\n",
      "  1.7156181  -1.1471297  -4.062592    2.381697   -1.3052199  -3.5199\n",
      "  0.9560087  -0.9491826  -1.0869992   2.902425    1.017673    3.1928408\n",
      " -0.01884623  4.108005   -3.5263915  -0.14417349  0.80200005  0.18797345\n",
      "  1.0731349   0.8374753   0.34341514 -2.5165257  -4.256272   -1.3647552\n",
      "  1.1712185  -2.0755334  -0.4080942   2.4889903   0.3853874   3.004748\n",
      "  0.1652333  -0.05391023  1.722412    0.5934981   0.6993224  -1.8594548\n",
      "  0.5135185   1.2517238   0.6661824   2.1260927   0.09025025  3.2303333\n",
      " -0.16979624  1.4212387  -0.29873636  0.4331206  -1.3758315   1.4100392\n",
      " -2.7220445   3.444666   -2.4451938   1.708834    1.9306154   1.434042\n",
      "  4.3922386   1.7995788   0.44206542  2.8793476   2.9679463  -0.10791497\n",
      "  1.1513164   2.1885722  -1.1753069  -0.69164187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(embeddings[2])\n",
    "print(model['there'])\n",
    "#print(model['There'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(word2int[0])\n",
    "# for i in range(0,10):\n",
    "#     print(int2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'left', 'her', 'husband', 'he', 'killed', 'their', 'children', 'just', 'another', 'day', 'in', 'america']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#word_vectors = np.zeros([],dtype=float)\n",
    "#print(words2[0:100])\n",
    "print(sentences[1])\n",
    "print(len(sentences[1]))\n",
    "#print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04204395 -2.184356   -1.8329806  ... -0.6191215   1.1494336\n",
      "   0.1696423 ]\n",
      " [-1.6818107   0.13794908 -1.2052062  ...  1.0178206   1.2658703\n",
      "   0.9165124 ]\n",
      " [ 0.6550478  -0.68891364 -1.1917928  ... -1.6304047   0.697925\n",
      "   2.8708901 ]\n",
      " ...\n",
      " [-2.179049    0.0059576  -0.39542943 ... -1.3494959   0.05578793\n",
      "  -0.8808921 ]\n",
      " [ 1.1458899   0.71995765 -1.3256361  ...  2.1202245   0.10713822\n",
      "   0.65151864]\n",
      " [-1.7001518  -1.1210458  -0.04484557 ... -2.0321565   0.22813459\n",
      "  -0.35970476]]\n",
      "0\n",
      "1\n",
      "[-0.04204395 -2.184356   -1.8329806   2.1544557   3.6301565  -0.9784656\n",
      " -2.2373903   2.2429914  -0.64196104 -1.3698995   0.42327255 -1.1105772\n",
      " -1.5183218   0.6469478  -0.30586     0.93902755  4.018184    1.2049829\n",
      "  0.18615219  1.6285034  -0.8934569  -3.2473712   2.0741017  -1.1492478\n",
      " -1.8982716  -0.63891333 -0.2758937  -2.1492312   3.7780826   1.3753589\n",
      " -0.17512439  1.0000547   0.6103615  -0.10518404  1.2457719  -0.28399262\n",
      " -0.6646198  -1.792475    0.5346496   4.1433077  -0.0472825  -3.5070083\n",
      "  2.1891932  -0.96867317 -3.045534    1.3023813   4.0290065   0.06412987\n",
      " -0.28437483  2.463548   -0.72060513 -2.2231221  -0.5905945   2.2191322\n",
      "  2.3674085  -0.08745275  1.5813162  -2.9955633   0.13179539  1.3022853\n",
      "  1.3902141   1.3540457   3.492437   -0.38254157 -0.3753863  -0.05264942\n",
      " -1.8482199  -1.948886    1.3234863   3.3526692  -3.4459014   3.0713372\n",
      " -0.44860488  0.4843159  -0.40379706  3.684199   -4.271357   -0.22918227\n",
      " -1.852486    1.6934091  -2.7945566   0.5494625  -0.51152235 -1.8658999\n",
      "  1.0056518   1.3749154   1.162095   -1.5384752  -0.71363086  2.4637206\n",
      "  0.02871158  2.8198516  -0.62104046  0.42381325 -1.7950906  -0.12780987\n",
      "  0.9667396  -0.6191215   1.1494336   0.1696423 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(model[sentences[1]])\n",
    "print(0%2)\n",
    "print(1%2)\n",
    "print(model['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "temp = []\n",
    "for i in range(0,len(sentences)):\n",
    "    for j in range(0,len(sentences[i])):\n",
    "        temp.append(model[sentences[i][j]])\n",
    "    if(i%2!=0):\n",
    "        X.append(temp)\n",
    "        temp = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(200832,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))    # Should be sum of no of words in sentences 1 and 2 (Headline and description for first training example)\n",
    "# Output = 27 (14+13)  Training data ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = []\n",
    "# test2 = []\n",
    "# test.append(model[sentences[0][0]])\n",
    "# print(test)\n",
    "# test.append(model[sentences[0][1]])\n",
    "# print(test)\n",
    "# test2.append(test)\n",
    "# print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "256\n",
      "58142\n"
     ]
    }
   ],
   "source": [
    "print(len(X[3443]))\n",
    "max1 = 0\n",
    "for i in range(0,len(X)):\n",
    "    if(len(X[i])>max1):\n",
    "        max1 = len(X[i])\n",
    "        pos = i\n",
    "print(max1)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116284\n"
     ]
    }
   ],
   "source": [
    "print(58142*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences[116285]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0,len(X)):\n",
    "    if(len(X[i])>100):\n",
    "        count = count + 1\n",
    "print(count)\n",
    "\n",
    "#  Sequence length : Length of each training example\n",
    "#  Sequence length is varying from 1 to 250, we have to choose a dimension and accordingly all training exapmles would be\n",
    "#  padded or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_new = keras.preprocessing.sequence.pad_sequences(sequences=X, maxlen=100, dtype='float32', padding='post', truncating='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#print(X[0])\n",
    "#print(X_new[0])\n",
    "#print(len(X_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_new\",X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_new_text\",X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------One-hot labels generation------------------###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###--------------------------CNN model-------------------------###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
