{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import math\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",dtype=object,na_values=str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV'\n",
      "  'She left her husband. He killed their children. Just another day in America.']\n",
      " [\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\"\n",
      "  'Of course it has a song.']\n",
      " ['Hugh Grant Marries For The First Time At Age 57'\n",
      "  'The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.']\n",
      " [\"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\"\n",
      "  'The actor gives Dems an ass-kicking for not fighting hard enough against Donald Trump.']]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(data[:,2:4])\n",
    "y = np.array(data[:,1])\n",
    "print(x[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = []\n",
    "for i in range(0,x.shape[0]):\n",
    "    for sent in x[i]:\n",
    "        if np.nan_to_num(sent) == 0:     # If short description is missing\n",
    "            corpus_raw.append(\"NA\")      # Necessary, otherwise while feeding training data into NN can cause mismatch\n",
    "        elif np.nan_to_num(sent) != 0:\n",
    "            corpus_raw.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV', 'She left her husband. He killed their children. Just another day in America.', \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\", 'Of course it has a song.', 'Hugh Grant Marries For The First Time At Age 57']\n",
      "(401664,)\n",
      "<class 'str'>\n",
      "NA\n"
     ]
    }
   ],
   "source": [
    "print(corpus_raw[0:5])\n",
    "print(np.shape(corpus_raw))\n",
    "print(type(corpus_raw[24199]))\n",
    "#words = corpus_raw[0].split()\n",
    "#print(words)\n",
    "print(np.nan_to_num(corpus_raw[24199]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words\n",
    "words2 = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "i = 0\n",
    "for sent in corpus_raw:\n",
    "    #words.append(sent.split())\n",
    "    #i = i + 1\n",
    "    #print(i)\n",
    "    for word in tokenizer.tokenize(sent):\n",
    "        words2.append(word.lower())\n",
    "    \n",
    "# for i in range(0,len(corpus_raw)):\n",
    "#     for word in corpus_raw[i].split():\n",
    "#         if word != '.':       # because we don't want to treat . as a word\n",
    "#             words.append(word)\n",
    "#print(type(words))\n",
    "#words = np.array(words)\n",
    "#print(type(words))\n",
    "#print(len(words))\n",
    "#print(words[0])\n",
    "\n",
    "Vocab = set(words2)    # so that all duplicate words are removed\n",
    "vocab = list(Vocab)\n",
    "# vocab = []\n",
    "# for j in words:\n",
    "#     if j not in vocab:\n",
    "#         vocab.append(j)\n",
    "\n",
    "\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(vocab)  # gives the total number of unique words\n",
    "\n",
    "for i,word in enumerate(vocab):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists of words\n",
    "words = []\n",
    "sentences = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "i = 0\n",
    "for sent in corpus_raw:\n",
    "    #words.append(sent.split())\n",
    "    #i = i + 1\n",
    "    #print(i)\n",
    "    for word in tokenizer.tokenize(sent):\n",
    "        words.append(word.lower())\n",
    "    sentences.append(words)\n",
    "    words = []\n",
    "# for i in range(0,len(corpus_raw)):\n",
    "#     for word in corpus_raw[i].split():\n",
    "#         if word != '.':       # because we don't want to treat . as a word\n",
    "#             words.append(word)\n",
    "#print(type(words))\n",
    "#words = np.array(words)\n",
    "#print(type(words))\n",
    "#print(len(words))\n",
    "#print(words[0])\n",
    "\n",
    "#Vocab = set(words)    # so that all duplicate words are removed\n",
    "#vocab = list(Vocab)\n",
    "# vocab = []\n",
    "# for j in words:\n",
    "#     if j not in vocab:\n",
    "#         vocab.append(j)\n",
    "\n",
    "\n",
    "# word2int = {}\n",
    "# int2word = {}\n",
    "# vocab_size = len(vocab)  # gives the total number of unique words\n",
    "\n",
    "# for i,word in enumerate(vocab):\n",
    "#     word2int[word] = i\n",
    "#     int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "86718\n",
      "[]\n",
      "<class 'list'>\n",
      "86718\n",
      "86718\n",
      "401664\n",
      "[['there', 'were', '2', 'mass', 'shootings', 'in', 'texas', 'last', 'week', 'but', 'only', '1', 'on', 'tv'], ['she', 'left', 'her', 'husband', 'he', 'killed', 'their', 'children', 'just', 'another', 'day', 'in', 'america'], ['will', 'smith', 'joins', 'diplo', 'and', 'nicky', 'jam', 'for', 'the', '2018', 'world', 'cup', 's', 'official', 'song'], ['of', 'course', 'it', 'has', 'a', 'song'], ['hugh', 'grant', 'marries', 'for', 'the', 'first', 'time', 'at', 'age', '57']]\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(vocab_size)\n",
    "print(words[0:100])\n",
    "print(type(vocab))\n",
    "#Vocab = list(vocab)\n",
    "#print(vocab[0:100])\n",
    "# for i in range(0,5):\n",
    "#     print(i)\n",
    "# for sent in corpus_raw:\n",
    "#     print(type(sent))\n",
    "print(len(word2int))\n",
    "print(len(int2word))\n",
    "#print(int2word)\n",
    "print(len(sentences))\n",
    "print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = Word2Vec(sentences, size=100, window=5, min_count=0,workers=10,sg=0)\n",
    "model_word2vec.train(sentences,total_examples=len(sentences),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=86718, size=100, alpha=0.025)\n",
      "[ 0.15428673  0.3032078  -1.1721619   1.874103   -1.5606085  -0.24525835\n",
      "  2.2021377  -2.475612   -1.7615445   2.326912   -0.34606314 -1.062049\n",
      " -2.2459078  -0.15821838 -0.0905036   0.8615143  -2.3459883  -1.5937464\n",
      " -0.3024151   0.47964537 -0.29445362  0.11982829  0.3789861   1.4195501\n",
      " -0.38800597 -0.37164438  1.7658195  -0.5426802   0.27196807 -0.8482216\n",
      "  0.53250164 -0.33730203 -0.6293161  -0.3897586   2.6819797  -0.40592122\n",
      " -0.14170778 -0.66269076  0.7727368  -2.8266966  -0.35886714 -0.7655429\n",
      " -2.1962283  -2.2179437  -2.791443   -1.0361419   1.4445623   0.84580624\n",
      " -0.05743724 -0.4303834  -0.47139132  0.29042226 -1.8581903  -1.1161158\n",
      " -0.01754724  1.4625732   2.1342294  -0.40597525 -0.37957555  0.5402142\n",
      " -0.8409273   0.12632468  0.8405126   1.2006891   2.2413278  -0.64666384\n",
      " -0.3393913   0.21993287 -0.9356112   0.19827822 -0.7891616  -1.3792859\n",
      "  0.51050097 -0.04699465  0.51752055 -0.20769632  0.81789476 -0.4723141\n",
      " -2.1866217  -0.57456815  1.8308525   0.36720723 -2.227297    2.3169405\n",
      "  1.1242387  -1.961003   -0.9348298   0.7909439  -0.55487615 -1.7964721\n",
      "  0.3012279   0.39903513  0.7006815   2.5910192  -1.6795545   1.0464658\n",
      "  0.18418832  0.47919834 -0.67648983 -2.7292597 ]\n",
      "[('shooting', 0.7852242588996887), ('killings', 0.7188578844070435), ('shooters', 0.6943228244781494), ('incarceration', 0.6891462206840515), ('massacre', 0.6837863326072693), ('newtown', 0.6685141324996948), ('mass', 0.6398292779922485), ('edgartown', 0.6281864643096924), ('aurora', 0.6132979989051819), ('rampage', 0.6017170548439026)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model_word2vec)\n",
    "print(model_word2vec['killing'])\n",
    "print(model_word2vec.wv.most_similar(\"shootings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.save(\"Saved_model_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=86718, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"Saved_model_word2vec\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "embeddings = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(86718, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02552463 -0.00969797 -0.00351117 -0.04741092  0.02596882 -0.02935318\n",
      "  0.03597238 -0.02313362  0.01829359 -0.0232755  -0.02904185  0.04753352\n",
      "  0.0189011  -0.01573795  0.00749357 -0.07616836  0.03188851  0.01264954\n",
      " -0.01731667 -0.02391621 -0.03672855 -0.08540228  0.00291198  0.0755619\n",
      " -0.16540246  0.02263342  0.12364018 -0.01391778 -0.02447489 -0.05229867\n",
      " -0.0203733  -0.01985734  0.15629353  0.0229524   0.05805507  0.03726301\n",
      "  0.05904296 -0.02795932  0.00524378 -0.05562707  0.00797274 -0.00135408\n",
      " -0.00103894 -0.09506186  0.06513759 -0.0862865   0.0346003   0.04814643\n",
      " -0.02115432  0.0366511   0.12914634  0.04191586  0.04918176  0.05426066\n",
      "  0.00619331 -0.00441547  0.05266495 -0.03947406  0.08843453 -0.05172638\n",
      " -0.11560268 -0.02359299 -0.02231005  0.03335159  0.02589093  0.07076951\n",
      " -0.00648663  0.07763797 -0.02239532 -0.0327848   0.03794491 -0.03568732\n",
      " -0.05356612  0.00056316 -0.08908082 -0.06410812 -0.09889576  0.0821877\n",
      " -0.03472503  0.05828639  0.08049561 -0.01130247 -0.04226131  0.02871085\n",
      "  0.02690672  0.00419515  0.12115051  0.08125009  0.01552579 -0.04646249\n",
      " -0.10196626 -0.06812107 -0.01622712 -0.04502429 -0.07881888  0.00605925\n",
      " -0.02771248 -0.05730564 -0.08271077  0.07431076]\n",
      "[-1.8612489  -0.2885676  -1.4941822   3.2265735  -0.08313849 -3.126247\n",
      "  0.22613381 -0.74041677 -0.41081485  2.5019407  -1.2451053   0.22269614\n",
      "  0.16604857  1.6952585   2.647251    1.6176267   2.55638    -0.7695693\n",
      " -2.9931254   1.4478403  -3.0225234  -2.7241569  -0.3744455   0.5215901\n",
      " -3.2990131   2.18888    -1.9480959   3.027542   -0.06710951  1.9445344\n",
      "  0.49774876  1.0684828  -1.189954   -2.544733    0.57041895 -3.3660605\n",
      "  1.7156181  -1.1471297  -4.062592    2.381697   -1.3052199  -3.5199\n",
      "  0.9560087  -0.9491826  -1.0869992   2.902425    1.017673    3.1928408\n",
      " -0.01884623  4.108005   -3.5263915  -0.14417349  0.80200005  0.18797345\n",
      "  1.0731349   0.8374753   0.34341514 -2.5165257  -4.256272   -1.3647552\n",
      "  1.1712185  -2.0755334  -0.4080942   2.4889903   0.3853874   3.004748\n",
      "  0.1652333  -0.05391023  1.722412    0.5934981   0.6993224  -1.8594548\n",
      "  0.5135185   1.2517238   0.6661824   2.1260927   0.09025025  3.2303333\n",
      " -0.16979624  1.4212387  -0.29873636  0.4331206  -1.3758315   1.4100392\n",
      " -2.7220445   3.444666   -2.4451938   1.708834    1.9306154   1.434042\n",
      "  4.3922386   1.7995788   0.44206542  2.8793476   2.9679463  -0.10791497\n",
      "  1.1513164   2.1885722  -1.1753069  -0.69164187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(embeddings[2])\n",
    "print(model['there'])\n",
    "#print(model['There'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(word2int[0])\n",
    "# for i in range(0,10):\n",
    "#     print(int2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'left', 'her', 'husband', 'he', 'killed', 'their', 'children', 'just', 'another', 'day', 'in', 'america']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "#word_vectors = np.zeros([],dtype=float)\n",
    "#print(words2[0:100])\n",
    "print(sentences[1])\n",
    "print(len(sentences[1]))\n",
    "#print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04204395 -2.184356   -1.8329806  ... -0.6191215   1.1494336\n",
      "   0.1696423 ]\n",
      " [-1.6818107   0.13794908 -1.2052062  ...  1.0178206   1.2658703\n",
      "   0.9165124 ]\n",
      " [ 0.6550478  -0.68891364 -1.1917928  ... -1.6304047   0.697925\n",
      "   2.8708901 ]\n",
      " ...\n",
      " [-2.179049    0.0059576  -0.39542943 ... -1.3494959   0.05578793\n",
      "  -0.8808921 ]\n",
      " [ 1.1458899   0.71995765 -1.3256361  ...  2.1202245   0.10713822\n",
      "   0.65151864]\n",
      " [-1.7001518  -1.1210458  -0.04484557 ... -2.0321565   0.22813459\n",
      "  -0.35970476]]\n",
      "0\n",
      "1\n",
      "[-0.04204395 -2.184356   -1.8329806   2.1544557   3.6301565  -0.9784656\n",
      " -2.2373903   2.2429914  -0.64196104 -1.3698995   0.42327255 -1.1105772\n",
      " -1.5183218   0.6469478  -0.30586     0.93902755  4.018184    1.2049829\n",
      "  0.18615219  1.6285034  -0.8934569  -3.2473712   2.0741017  -1.1492478\n",
      " -1.8982716  -0.63891333 -0.2758937  -2.1492312   3.7780826   1.3753589\n",
      " -0.17512439  1.0000547   0.6103615  -0.10518404  1.2457719  -0.28399262\n",
      " -0.6646198  -1.792475    0.5346496   4.1433077  -0.0472825  -3.5070083\n",
      "  2.1891932  -0.96867317 -3.045534    1.3023813   4.0290065   0.06412987\n",
      " -0.28437483  2.463548   -0.72060513 -2.2231221  -0.5905945   2.2191322\n",
      "  2.3674085  -0.08745275  1.5813162  -2.9955633   0.13179539  1.3022853\n",
      "  1.3902141   1.3540457   3.492437   -0.38254157 -0.3753863  -0.05264942\n",
      " -1.8482199  -1.948886    1.3234863   3.3526692  -3.4459014   3.0713372\n",
      " -0.44860488  0.4843159  -0.40379706  3.684199   -4.271357   -0.22918227\n",
      " -1.852486    1.6934091  -2.7945566   0.5494625  -0.51152235 -1.8658999\n",
      "  1.0056518   1.3749154   1.162095   -1.5384752  -0.71363086  2.4637206\n",
      "  0.02871158  2.8198516  -0.62104046  0.42381325 -1.7950906  -0.12780987\n",
      "  0.9667396  -0.6191215   1.1494336   0.1696423 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(model[sentences[1]])\n",
    "print(0%2)\n",
    "print(1%2)\n",
    "print(model['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "temp = []\n",
    "for i in range(0,len(sentences)):\n",
    "    for j in range(0,len(sentences[i])):\n",
    "        temp.append(model[sentences[i][j]])\n",
    "    if(i%2!=0):\n",
    "        X.append(temp)\n",
    "        temp = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-40d3c61602d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))    # Should be sum of no of words in sentences 1 and 2 (Headline and description for first training example)\n",
    "# Output = 27 (14+13)  Training data ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = []\n",
    "# test2 = []\n",
    "# test.append(model[sentences[0][0]])\n",
    "# print(test)\n",
    "# test.append(model[sentences[0][1]])\n",
    "# print(test)\n",
    "# test2.append(test)\n",
    "# print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "256\n",
      "58142\n"
     ]
    }
   ],
   "source": [
    "print(len(X[3443]))\n",
    "max1 = 0\n",
    "for i in range(0,len(X)):\n",
    "    if(len(X[i])>max1):\n",
    "        max1 = len(X[i])\n",
    "        pos = i\n",
    "print(max1)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116284\n"
     ]
    }
   ],
   "source": [
    "print(58142*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences[116285]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0,len(X)):\n",
    "    if(len(X[i])>100):\n",
    "        count = count + 1\n",
    "print(count)\n",
    "\n",
    "#  Sequence length : Length of each training example\n",
    "#  Sequence length is varying from 1 to 250, we have to choose a dimension and accordingly all training exapmles would be\n",
    "#  padded or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_new = keras.preprocessing.sequence.pad_sequences(sequences=X, maxlen=100, dtype='float32', padding='post', truncating='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#print(X[0])\n",
    "#print(X_new[0])\n",
    "#print(len(X_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(200832, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_new))\n",
    "print(np.shape(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_new\",X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"/media/shubham/1A2A3CBF2A3C99A9/Academics/Self/Netapp-Data-Challenge-Kshitij-storage/X_new.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"X_new_text\",X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------One-hot labels generation------------------###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200832\n",
      "['crime' 'entertainment' 'entertainment' 'entertainment']\n",
      "['culture & arts', 'world news', 'money', 'tech', 'queer voices', 'green', 'divorce', 'worldpost', 'taste', 'comedy', 'arts & culture', 'the worldpost', 'weddings', 'business', 'entertainment', 'travel', 'media', 'women', 'arts', 'sports', 'politics', 'food & drink', 'environment', 'science', 'college', 'fifty', 'impact', 'style', 'style & beauty', 'healthy living', 'parenting', 'wellness', 'weird news', 'black voices', 'home & living', 'crime', 'religion', 'latino voices', 'education', 'good news', 'parents']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(y))\n",
    "print(y[0:4])\n",
    "print(list(set(y)))\n",
    "print(len(list(set(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "y_labels = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_labels[0:20])\n",
    "one_hot_labels = tf.keras.utils.to_categorical(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200832, 41)\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(one_hot_labels))\n",
    "print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('labels.csv', 'w') as csvfile:\n",
    "    wr = csv.writer(csvfile)\n",
    "    wr.writerow(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"one_hot_labels.csv\", one_hot_labels, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###--------------------------CNN model-------------------------###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(seq_length, embedding_size, n_y):\n",
    "    \n",
    "#     Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "#     Arguments:\n",
    "#     n_H0 -- scalar, height of an input image\n",
    "#     n_W0 -- scalar, width of an input image\n",
    "#     n_C0 -- scalar, number of channels of the input\n",
    "#     n_y -- scalar, number of classes\n",
    "        \n",
    "#     Returns:\n",
    "#     X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "#     Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \n",
    "\n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    X = tf.placeholder(dtype = tf.float32, shape=(None,seq_length,embedding_size,1))\n",
    "    Y = tf.placeholder(dtype = tf.float32, shape=(None,n_y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_parameters(filter_size,embedding_size,num_filters):\n",
    "#     # Initializes weight parameters\n",
    "#     W = tf.get_variable(\"W\",[filter_size,embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "#     return W\n",
    "\n",
    "def initialize_parameters(filter_sizes,embedding_size,num_filters):\n",
    "    # Initializes weight parameters\n",
    "    W1 = tf.get_variable(\"W1\",[filter_sizes[0],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    W2 = tf.get_variable(\"W2\",[filter_sizes[1],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    W3 = tf.get_variable(\"W3\",[filter_sizes[2],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    W4 = tf.get_variable(\"W4\",[filter_sizes[3],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\": W3,\n",
    "                  \"W4\": W4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_propagation(X,filter_sizes,embedding_size,num_filters,seq_length):\n",
    "#     P2 = []\n",
    "#     for filter_size in filter_sizes:\n",
    "#         W = initialize_parameters(filter_size,embedding_size,num_filters)\n",
    "#         Z = tf.nn.conv2d(X,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "#         A = tf.nn.relu(Z)\n",
    "#         P = tf.nn.max_pool(A,ksize=[1,seq_length-filter_size+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "#         P2.append(P)\n",
    "#     Z2 = tf.contrib.layers.fully_connected(P2,41,activation_fn = None)\n",
    "#     return Z2\n",
    "\n",
    "def forward_propagation(X,filter_sizes,embedding_size,num_filters,seq_length,parameters):\n",
    "    P = []\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    #W1 = initialize_parameters(filter_sizes[0],embedding_size,num_filters)\n",
    "    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1,ksize=[1,seq_length-filter_sizes[0]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P1)\n",
    "    \n",
    "    #W2 = initialize_parameters(filter_sizes[1],embedding_size,num_filters)\n",
    "    Z2 = tf.nn.conv2d(X,W2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2,ksize=[1,seq_length-filter_sizes[1]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P2)\n",
    "    \n",
    "    #W3 = initialize_parameters(filter_sizes[2],embedding_size,num_filters)\n",
    "    Z3 = tf.nn.conv2d(X,W3,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    P3 = tf.nn.max_pool(A3,ksize=[1,seq_length-filter_sizes[2]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P3)\n",
    "    \n",
    "    #W4 = initialize_parameters(filter_sizes[3],embedding_size,num_filters)\n",
    "    Z4 = tf.nn.conv2d(X,W4,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    P4 = tf.nn.max_pool(A4,ksize=[1,seq_length-filter_sizes[3]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P4)\n",
    "    \n",
    "    Z5 = tf.contrib.layers.fully_connected(P4,41,activation_fn = None)\n",
    "    return Z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z5, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z5 -- output of forward propagation (output of the last LINEAR unit), of shape (number of examples,41)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z5\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z5, labels = Y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 8, seed = 0):\n",
    "    np.random.seed(seed)            \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((41,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.007,\n",
    "          num_epochs = 10, minibatch_size = 8, print_cost = True):\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    \n",
    "    ## To be used if not using stochastic\n",
    "    #(m, seq_length, embedding_size,nc) = X_train.shape             \n",
    "    ##-----------------------------------------###\n",
    "    \n",
    "    \n",
    "    ## To be used if using Stochastic ##\n",
    "    m = X_train.shape[0]\n",
    "    seq_length = X_train.shape[2]\n",
    "    embedding_size = X_train.shape[3]\n",
    "    nc = X_train.shape[4]\n",
    "    ##------------------------------------####\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_y = Y_train.shape[2]            # 2 - stochastic;  1 - otherwise                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    filter_sizes = [2,3,5,7]\n",
    "    num_filters = 5\n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(seq_length, embedding_size, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(filter_sizes,embedding_size,num_filters)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "\n",
    "    Z5 = forward_propagation(X,filter_sizes,embedding_size,num_filters,seq_length,parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z5, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "           # _, temp_cost = sess.run([optimizer, cost], feed_dict = {X:X_train, Y:Y_train})  #Batch Gradient Descent\n",
    "\n",
    "#             minibatch_cost = 0.\n",
    "#             num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "#             seed = seed + 1\n",
    "#             minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "#             for minibatch in minibatches:\n",
    "\n",
    "#                 # Select a minibatch\n",
    "#                 (minibatch_X, minibatch_Y) = minibatch\n",
    "#                 # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "#                 # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "#                 ### START CODE HERE ### (1 line)\n",
    "#                 _ , temp_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, Y: minibatch_Y})     # mini_batch gradieent descent\n",
    "#                 ### END CODE HERE ###\n",
    "                \n",
    "#                 minibatch_cost += temp_cost / num_minibatches\n",
    "            stochastic_cost=0    \n",
    "            for i in range(0,m):\n",
    "                _, temp_cost = sess.run([optimizer, cost], feed_dict = {X:X_train[i], Y:Y_train[i]}) \n",
    "                stochastic_cost += temp_cost/m\n",
    "                if(i%10==0):\n",
    "                    print(\"Cost after\",i,\"iterations =\",stochastic_cost)\n",
    "                \n",
    "            # Print the cost every epoch\n",
    "#             if print_cost == True and epoch % 5 == 0:\n",
    "#                 print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "#             if print_cost == True and epoch % 1 == 0:\n",
    "#                 costs.append(minibatch_cost)\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, stochastic_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(stochastic_cost)\n",
    "#             if print_cost == True and epoch % 5 == 0:\n",
    "#                 print (\"Cost after epoch %i: %f\" % (epoch, temp_cost))\n",
    "#             if print_cost == True and epoch % 1 == 0:\n",
    "#                 costs.append(temp_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z5, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        #accuracy = tf.Print(accuracy, [accuracy], message=\"Accuracy: \")\n",
    "        #print(sess.run(accuracy))\n",
    "        train_accuracy = accuracy.eval({X: np.squeeze(X_train,axis=1), Y: np.squeeze(Y_train,axis=1)})\n",
    "        test_accuracy = accuracy.eval({X: np.squeeze(X_test,axis=1), Y: np.squeeze(Y_test,axis=1)})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "        \n",
    "        #precision = tf.metrics.precision(Y_test,correct_prediction)\n",
    "        #recall = tf.metrics.recall(Y_test,correct_prediction)\n",
    "        \n",
    "        #print(\"Precision =\",precision)\n",
    "        #print(\"Recall=\",recall)\n",
    "        \n",
    "        #F1_score_sklearn = f1_score()\n",
    "        #F1_score_tf = tf.contrib.metrics.f1_score(Y_test,predictions) \n",
    "        #print(\"F1_score=\",F1_score_tf)\n",
    "                \n",
    "        #return train_accuracy, test_accuracy, predict_op,parameters\n",
    "        return predict_op,correct_prediction,parameters,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200832, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "#X = X_new    # Comment out if loaded from X_new.npy, otherwise run\n",
    "Y = one_hot_labels\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200832, 41)\n",
      "[-1.8612489  -0.2885676  -1.4941822   3.2265735  -0.08313849 -3.126247\n",
      "  0.22613381 -0.74041677 -0.41081485  2.5019407  -1.2451053   0.22269614\n",
      "  0.16604857  1.6952585   2.647251    1.6176267   2.55638    -0.7695693\n",
      " -2.9931254   1.4478403  -3.0225234  -2.7241569  -0.3744455   0.5215901\n",
      " -3.2990131   2.18888    -1.9480959   3.027542   -0.06710951  1.9445344\n",
      "  0.49774876  1.0684828  -1.189954   -2.544733    0.57041895 -3.3660605\n",
      "  1.7156181  -1.1471297  -4.062592    2.381697   -1.3052199  -3.5199\n",
      "  0.9560087  -0.9491826  -1.0869992   2.902425    1.017673    3.1928408\n",
      " -0.01884623  4.108005   -3.5263915  -0.14417349  0.80200005  0.18797345\n",
      "  1.0731349   0.8374753   0.34341514 -2.5165257  -4.256272   -1.3647552\n",
      "  1.1712185  -2.0755334  -0.4080942   2.4889903   0.3853874   3.004748\n",
      "  0.1652333  -0.05391023  1.722412    0.5934981   0.6993224  -1.8594548\n",
      "  0.5135185   1.2517238   0.6661824   2.1260927   0.09025025  3.2303333\n",
      " -0.16979624  1.4212387  -0.29873636  0.4331206  -1.3758315   1.4100392\n",
      " -2.7220445   3.444666   -2.4451938   1.708834    1.9306154   1.434042\n",
      "  4.3922386   1.7995788   0.44206542  2.8793476   2.9679463  -0.10791497\n",
      "  1.1513164   2.1885722  -1.1753069  -0.69164187]\n",
      "(41, 200832)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Y))\n",
    "print(X[0][0])\n",
    "Y = Y.T\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(Y))\n",
    "Y = one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n",
    "# Giving memory error\n",
    "# X_train = X[0:150000]\n",
    "# X_test = X[150000:]\n",
    "# Y_train = Y[0:150000]\n",
    "# Y_test = Y[150000:]\n",
    "\n",
    "X_train = X[5000:5500]\n",
    "X_test = X[8000:8200]\n",
    "Y_train = Y[5000:5500]\n",
    "Y_test = Y[8000:8200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train = Y_train.T\n",
    "# Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train,axis=3)\n",
    "X_test = np.expand_dims(X_test,axis=3)\n",
    "\n",
    "X_train = np.expand_dims(X_train,axis=1)      #Only to be used, if using stochastic gradient descent\n",
    "X_test = np.expand_dims(X_test,axis=1)        #Only to be used, if using stochastic gradient descent\n",
    "Y_train = np.expand_dims(Y_train,axis=1)\n",
    "Y_test = np.expand_dims(Y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1, 100, 100, 1)\n",
      "(500, 1, 41)\n",
      "(1, 100, 100, 1)\n",
      "(200, 1, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(X_train[0]))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c963249bbeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#_, _, predictions,parameters = model(X_train, Y_train, X_test, Y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#_, _, predictions,parameters = model(X_train, Y_train, X_test, Y_test)\n",
    "predict_op,correct_prediction,parameters,accuracy = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 j= 10\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=10\n",
    "print(\"i =\",i,\"j=\",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100, 41)\n",
      "(?, 100, 41)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(predict_op))\n",
    "print(np.shape(correct_prediction))\n",
    "print(np.shape(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(100, 41), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(predict_op[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Print:0' shape=(100, 41) dtype=int64>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Print(predict_op[0],[predict_op[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,41]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,41], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-72-0c963249bbeb>\", line 2, in <module>\n    predict_op,correct_prediction,parameters,accuracy = model(X_train, Y_train, X_test, Y_test)\n  File \"<ipython-input-71-c546fa1708ea>\", line 26, in model\n    X, Y = create_placeholders(seq_length, embedding_size, n_y)\n  File \"<ipython-input-29-d6414442e892>\", line 18, in create_placeholders\n    Y = tf.placeholder(dtype = tf.float32, shape=(None,n_y))\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1808, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4848, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,41]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,41], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,41]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,41], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-45f626ba5b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    935\u001b[0m   \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m   \"\"\"\n\u001b[0;32m--> 937\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5178\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5180\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,41]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,41], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-72-0c963249bbeb>\", line 2, in <module>\n    predict_op,correct_prediction,parameters,accuracy = model(X_train, Y_train, X_test, Y_test)\n  File \"<ipython-input-71-c546fa1708ea>\", line 26, in model\n    X, Y = create_placeholders(seq_length, embedding_size, n_y)\n  File \"<ipython-input-29-d6414442e892>\", line 18, in create_placeholders\n    Y = tf.placeholder(dtype = tf.float32, shape=(None,n_y))\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1808, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4848, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/shubham/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,41]\n\t [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[?,41], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.keras.backend.eval(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
